{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b684d1c-49d8-4a89-80ee-8f54c4f6fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mbrzozowski/projects/media_monitoring/roberta_for_longer_texts\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ..\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f020349-c880-426d-a878-3089c79c9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from config import VISIBLE_GPUS\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= VISIBLE_GPUS\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.main import BERTClassificationModelWithPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77184f93-d4e0-449f-8a5e-c9b1103d077c",
   "metadata": {},
   "source": [
    "## Load data - sample of IMDB reviews in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bddeba-98da-4eeb-b214-16318539a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DATA_PATH = 'test/sample_data/sample_data_eng.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84ad322-1484-4cd8-bf44-9bd7dcdb7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data for tests\n",
    "df = pd.read_csv(SAMPLE_DATA_PATH)\n",
    "\n",
    "texts = df['sentence'].tolist() # list of texts\n",
    "labels = df['target'].tolist() # list of 0/1 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1961058-55b7-456d-af4c-964c07b6cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie not knowing anything about it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK, don't let my summary fool you. This movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This should be re-titled \"The Curious Case Of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Those 2 points are dedicated the reasonable pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Following the success of the (awful) Gilligan'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>What if Marylin Monroe, Albert Einstein, Joe D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Such a film of beauty that it's hard to descri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>I saw this movie with my friend and we couldnt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>This is the best piece of film ever created It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>First of all, don't go into Revolver expecting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  target\n",
       "0     I saw this movie not knowing anything about it...       0\n",
       "1     OK, don't let my summary fool you. This movie ...       0\n",
       "2     This should be re-titled \"The Curious Case Of ...       0\n",
       "3     Those 2 points are dedicated the reasonable pe...       0\n",
       "4     Following the success of the (awful) Gilligan'...       0\n",
       "...                                                 ...     ...\n",
       "1995  What if Marylin Monroe, Albert Einstein, Joe D...       1\n",
       "1996  Such a film of beauty that it's hard to descri...       1\n",
       "1997  I saw this movie with my friend and we couldnt...       1\n",
       "1998  This is the best piece of film ever created It...       1\n",
       "1999  First of all, don't go into Revolver expecting...       1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5dcfa-bf7f-4202-a6b6-5ed256fbfd4d",
   "metadata": {},
   "source": [
    "## Divide to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1505f595-5990-4b90-8e75-eed4e4c4e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a129d-c4d1-465f-ae0f-8fd80318f45e",
   "metadata": {},
   "source": [
    "# Fit and predict methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d07f5-dad0-4612-9cf0-793716e79197",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39fde966-d26c-40e6-9c17-a1a108f7b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train accuracy: 0.7825, Train loss: 0.4733248513750732\n",
      "Epoch: 1, Train accuracy: 0.936875, Train loss: 0.2021335910446942\n",
      "Epoch: 2, Train accuracy: 0.963125, Train loss: 0.11570522608701139\n",
      "Epoch: 3, Train accuracy: 0.98375, Train loss: 0.07096196901286021\n",
      "Epoch: 4, Train accuracy: 0.986875, Train loss: 0.0511996485106647\n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "model = BERTClassificationModelWithPooling()\n",
    "# Fitting a model to training data for 5 epochs\n",
    "model.fit(X_train,y_train,epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d1e36-a3e5-4e81-885d-48b91a6148e6",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b707a272-422d-4c7d-b5d0-fefcaec5b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability for test set\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0904b3-7574-4861-bddf-a1dd4eee485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9981957077980042,\n",
       " 0.0014405354158952832,\n",
       " 0.9900174736976624,\n",
       " 0.0012359063839539886,\n",
       " 0.9980940222740173,\n",
       " 0.9989404678344727,\n",
       " 0.007049195002764463,\n",
       " 0.9982878565788269,\n",
       " 0.0022877631708979607,\n",
       " 0.42834532260894775,\n",
       " 0.0010692067444324493,\n",
       " 0.9988347887992859,\n",
       " 0.9978082776069641,\n",
       " 0.012943921610713005,\n",
       " 0.0023247345816344023,\n",
       " 0.001353072002530098,\n",
       " 0.0012805807637050748,\n",
       " 0.18986642360687256,\n",
       " 0.9973364472389221,\n",
       " 0.5747166872024536,\n",
       " 0.007323339115828276,\n",
       " 0.9988862872123718,\n",
       " 0.996353268623352,\n",
       " 0.5548359751701355,\n",
       " 0.9965223073959351,\n",
       " 0.002497371518984437,\n",
       " 0.5350319147109985,\n",
       " 0.00200158660300076,\n",
       " 0.0015786592848598957,\n",
       " 0.0015121130272746086,\n",
       " 0.0050605591386556625,\n",
       " 0.004486889112740755,\n",
       " 0.0011420610826462507,\n",
       " 0.0025090016424655914,\n",
       " 0.9987766146659851,\n",
       " 0.0015395948430523276,\n",
       " 0.5616910457611084,\n",
       " 0.9986227750778198,\n",
       " 0.9977723360061646,\n",
       " 0.004975558258593082,\n",
       " 0.7612091898918152,\n",
       " 0.999107301235199,\n",
       " 0.0021477288100868464,\n",
       " 0.00209933053702116,\n",
       " 0.9860948324203491,\n",
       " 0.1764865666627884,\n",
       " 0.0017747003585100174,\n",
       " 0.0025716153904795647,\n",
       " 0.0016198992962017655,\n",
       " 0.9989670515060425,\n",
       " 0.9976083636283875,\n",
       " 0.002298290142789483,\n",
       " 0.9972209334373474,\n",
       " 0.001861396012827754,\n",
       " 0.0014347266405820847,\n",
       " 0.009924005717039108,\n",
       " 0.9411113262176514,\n",
       " 0.971994161605835,\n",
       " 0.9988938570022583,\n",
       " 0.001726655405946076,\n",
       " 0.9984526038169861,\n",
       " 0.001713986974209547,\n",
       " 0.9986457228660583,\n",
       " 0.9985877275466919,\n",
       " 0.9985061287879944,\n",
       " 0.9989315867424011,\n",
       " 0.008649180643260479,\n",
       " 0.8927907943725586,\n",
       " 0.02351749874651432,\n",
       " 0.9949969053268433,\n",
       " 0.001590804080478847,\n",
       " 0.11761770397424698,\n",
       " 0.9919639825820923,\n",
       " 0.9906467795372009,\n",
       " 0.0015299305086955428,\n",
       " 0.0021503332536667585,\n",
       " 0.9986479878425598,\n",
       " 0.0016564942197874188,\n",
       " 0.0036200082395225763,\n",
       " 0.6656144261360168,\n",
       " 0.006959260441362858,\n",
       " 0.0126096336171031,\n",
       " 0.8828036785125732,\n",
       " 0.9987083673477173,\n",
       " 0.4981268048286438,\n",
       " 0.001453183125704527,\n",
       " 0.999011754989624,\n",
       " 0.004155125003308058,\n",
       " 0.0017680166056379676,\n",
       " 0.0036475355736911297,\n",
       " 0.9976874589920044,\n",
       " 0.02046157605946064,\n",
       " 0.017507392913103104,\n",
       " 0.003094710409641266,\n",
       " 0.9982209801673889,\n",
       " 0.7154374122619629,\n",
       " 0.0015596805606037378,\n",
       " 0.9951181411743164,\n",
       " 0.9989519119262695,\n",
       " 0.3021395206451416,\n",
       " 0.04566606879234314,\n",
       " 0.002414415590465069,\n",
       " 0.009864130057394505,\n",
       " 0.9854867458343506,\n",
       " 0.04239942878484726,\n",
       " 0.4572664797306061,\n",
       " 0.0015497347339987755,\n",
       " 0.0019361987942829728,\n",
       " 0.0029550958424806595,\n",
       " 0.001208304543979466,\n",
       " 0.46450257301330566,\n",
       " 0.9992125034332275,\n",
       " 0.994097113609314,\n",
       " 0.9955004453659058,\n",
       " 0.0011050296016037464,\n",
       " 0.0015471427468582988,\n",
       " 0.9984794855117798,\n",
       " 0.002385059604421258,\n",
       " 0.0036246241070330143,\n",
       " 0.9728959798812866,\n",
       " 0.5029441714286804,\n",
       " 0.9972342848777771,\n",
       " 0.8851486444473267,\n",
       " 0.001584589364938438,\n",
       " 0.0019241725094616413,\n",
       " 0.6298829317092896,\n",
       " 0.4308716356754303,\n",
       " 0.020378079265356064,\n",
       " 0.9920897483825684,\n",
       " 0.0021249407436698675,\n",
       " 0.9896731376647949,\n",
       " 0.9987874627113342,\n",
       " 0.004139779135584831,\n",
       " 0.17478017508983612,\n",
       " 0.0029950349126011133,\n",
       " 0.9721935391426086,\n",
       " 0.9932225942611694,\n",
       " 0.9989452958106995,\n",
       " 0.9988391995429993,\n",
       " 0.9849352836608887,\n",
       " 0.004422388505190611,\n",
       " 0.0014361542416736484,\n",
       " 0.41186168789863586,\n",
       " 0.003840119345113635,\n",
       " 0.9989415407180786,\n",
       " 0.002903410466387868,\n",
       " 0.0021814783103764057,\n",
       " 0.5112955570220947,\n",
       " 0.9568287134170532,\n",
       " 0.004131808876991272,\n",
       " 0.7482495903968811,\n",
       " 0.34724846482276917,\n",
       " 0.0021550017409026623,\n",
       " 0.9977580904960632,\n",
       " 0.5232330560684204,\n",
       " 0.0038810372352600098,\n",
       " 0.03204682841897011,\n",
       " 0.9989480376243591,\n",
       " 0.1752602756023407,\n",
       " 0.9985736608505249,\n",
       " 0.0038873846642673016,\n",
       " 0.01692493073642254,\n",
       " 0.002411805558949709,\n",
       " 0.9885426163673401,\n",
       " 0.9988757371902466,\n",
       " 0.9986565113067627,\n",
       " 0.9988911151885986,\n",
       " 0.9950525164604187,\n",
       " 0.008218239061534405,\n",
       " 0.5624524354934692,\n",
       " 0.9982798099517822,\n",
       " 0.08022449165582657,\n",
       " 0.002657974138855934,\n",
       " 0.666456937789917,\n",
       " 0.0016013309359550476,\n",
       " 0.9986805319786072,\n",
       " 0.0017379779601469636,\n",
       " 0.868402361869812,\n",
       " 0.003245195373892784,\n",
       " 0.9931592345237732,\n",
       " 0.9816486835479736,\n",
       " 0.7817708253860474,\n",
       " 0.9949986934661865,\n",
       " 0.9606080651283264,\n",
       " 0.00497070699930191,\n",
       " 0.06029096245765686,\n",
       " 0.9939759969711304,\n",
       " 0.989605188369751,\n",
       " 0.9982353448867798,\n",
       " 0.998439610004425,\n",
       " 0.0019529855344444513,\n",
       " 0.0016743545420467854,\n",
       " 0.00400738138705492,\n",
       " 0.001426942297257483,\n",
       " 0.6624864935874939,\n",
       " 0.03678280860185623,\n",
       " 0.9965707063674927,\n",
       " 0.01733759231865406,\n",
       " 0.002515128580853343,\n",
       " 0.49871590733528137,\n",
       " 0.998137354850769,\n",
       " 0.0012550904648378491,\n",
       " 0.00156494602560997,\n",
       " 0.004287810996174812,\n",
       " 0.002771934261545539,\n",
       " 0.002029531169682741,\n",
       " 0.9990454316139221,\n",
       " 0.9992031455039978,\n",
       " 0.9968769550323486,\n",
       " 0.0029104158747941256,\n",
       " 0.5042674541473389,\n",
       " 0.004693734459578991,\n",
       " 0.0032615524251013994,\n",
       " 0.002983917249366641,\n",
       " 0.014935621991753578,\n",
       " 0.0691274031996727,\n",
       " 0.9984182119369507,\n",
       " 0.47139251232147217,\n",
       " 0.0014365866081789136,\n",
       " 0.0027876573149114847,\n",
       " 0.0017564354930073023,\n",
       " 0.0018872162327170372,\n",
       " 0.005959969945251942,\n",
       " 0.996081531047821,\n",
       " 0.9987321496009827,\n",
       " 0.6710205078125,\n",
       " 0.49840888381004333,\n",
       " 0.0019524425733834505,\n",
       " 0.9955534338951111,\n",
       " 0.001375670195557177,\n",
       " 0.9989123344421387,\n",
       " 0.0021054737735539675,\n",
       " 0.999002993106842,\n",
       " 0.9914976954460144,\n",
       " 0.9931074976921082,\n",
       " 0.6737430095672607,\n",
       " 0.998921275138855,\n",
       " 0.006435141898691654,\n",
       " 0.8507901430130005,\n",
       " 0.0019219988025724888,\n",
       " 0.014828955754637718,\n",
       " 0.9984564781188965,\n",
       " 0.001381677226163447,\n",
       " 0.5401737689971924,\n",
       " 0.9980862736701965,\n",
       " 0.21927927434444427,\n",
       " 0.6975811719894409,\n",
       " 0.5176557898521423,\n",
       " 0.10929999500513077,\n",
       " 0.9982739686965942,\n",
       " 0.8161265254020691,\n",
       " 0.009772970341145992,\n",
       " 0.9962469935417175,\n",
       " 0.00243019824847579,\n",
       " 0.43301671743392944,\n",
       " 0.998814582824707,\n",
       " 0.013699627481400967,\n",
       " 0.0016999772051349282,\n",
       " 0.8875538110733032,\n",
       " 0.8571797013282776,\n",
       " 0.3632255494594574,\n",
       " 0.9983793497085571,\n",
       " 0.017488986253738403,\n",
       " 0.9988914132118225,\n",
       " 0.9978491067886353,\n",
       " 0.5277499556541443,\n",
       " 0.002282253699377179,\n",
       " 0.20034927129745483,\n",
       " 0.01758110336959362,\n",
       " 0.991595447063446,\n",
       " 0.9969242215156555,\n",
       " 0.9986690282821655,\n",
       " 0.0010225374717265368,\n",
       " 0.9976557493209839,\n",
       " 0.0018227110849693418,\n",
       " 0.6863882541656494,\n",
       " 0.0029908993747085333,\n",
       " 0.48538193106651306,\n",
       " 0.47687986493110657,\n",
       " 0.002361218212172389,\n",
       " 0.9941537976264954,\n",
       " 0.9985100626945496,\n",
       " 0.0034188013523817062,\n",
       " 0.9949849247932434,\n",
       " 0.9986611008644104,\n",
       " 0.0036345694679766893,\n",
       " 0.004135666880756617,\n",
       " 0.9987022876739502,\n",
       " 0.2234463393688202,\n",
       " 0.006259019486606121,\n",
       " 0.917401909828186,\n",
       " 0.9978621602058411,\n",
       " 0.6080292463302612,\n",
       " 0.9967555999755859,\n",
       " 0.00230629020370543,\n",
       " 0.0012943539768457413,\n",
       " 0.0037554327864199877,\n",
       " 0.7501685619354248,\n",
       " 0.9988817572593689,\n",
       " 0.0015237609623000026,\n",
       " 0.0034276065416634083,\n",
       " 0.003128648968413472,\n",
       " 0.9515892863273621,\n",
       " 0.9944597482681274,\n",
       " 0.0014183721505105495,\n",
       " 0.005971348378807306,\n",
       " 0.40868884325027466,\n",
       " 0.09390336275100708,\n",
       " 0.4994352161884308,\n",
       " 0.006769333966076374,\n",
       " 0.03958100825548172,\n",
       " 0.6142852902412415,\n",
       " 0.9988364577293396,\n",
       " 0.002599848434329033,\n",
       " 0.9971511960029602,\n",
       " 0.8137710690498352,\n",
       " 0.0014010273152962327,\n",
       " 0.9986627101898193,\n",
       " 0.9988032579421997,\n",
       " 0.9985812306404114,\n",
       " 0.0019492045976221561,\n",
       " 0.4828660488128662,\n",
       " 0.018323469907045364,\n",
       " 0.00514236930757761,\n",
       " 0.002818287815898657,\n",
       " 0.0012206778628751636,\n",
       " 0.9988847374916077,\n",
       " 0.002021530643105507,\n",
       " 0.9972641468048096,\n",
       " 0.0014016124187037349,\n",
       " 0.002763705560937524,\n",
       " 0.004314953461289406,\n",
       " 0.005639657378196716,\n",
       " 0.002594725228846073,\n",
       " 0.002336053643375635,\n",
       " 0.9989921450614929,\n",
       " 0.0018060582224279642,\n",
       " 0.0021029675845056772,\n",
       " 0.6112497448921204,\n",
       " 0.9975386261940002,\n",
       " 0.9964050054550171,\n",
       " 0.004625395406037569,\n",
       " 0.4213579297065735,\n",
       " 0.002617324935272336,\n",
       " 0.039114661514759064,\n",
       " 0.20535634458065033,\n",
       " 0.001596552785485983,\n",
       " 0.002926961751654744,\n",
       " 0.9913199543952942,\n",
       " 0.0012417897814884782,\n",
       " 0.9719621539115906,\n",
       " 0.008600997738540173,\n",
       " 0.9988033771514893,\n",
       " 0.14844271540641785,\n",
       " 0.18092797696590424,\n",
       " 0.992100715637207,\n",
       " 0.7711855173110962,\n",
       " 0.9987014532089233,\n",
       " 0.5269153118133545,\n",
       " 0.9979907274246216,\n",
       " 0.1430119276046753,\n",
       " 0.9785107374191284,\n",
       " 0.04187960922718048,\n",
       " 0.06275556981563568,\n",
       " 0.03700840100646019,\n",
       " 0.003550624242052436,\n",
       " 0.9875507354736328,\n",
       " 0.0014670324744656682,\n",
       " 0.9976471066474915,\n",
       " 0.0028566045220941305,\n",
       " 0.1643093228340149,\n",
       " 0.6625986695289612,\n",
       " 0.006671496666967869,\n",
       " 0.9990059733390808,\n",
       " 0.4364555776119232,\n",
       " 0.4945117235183716,\n",
       " 0.0013823898043483496,\n",
       " 0.5087176561355591,\n",
       " 0.998940646648407,\n",
       " 0.9990082383155823,\n",
       " 0.9987574815750122,\n",
       " 0.5003082156181335,\n",
       " 0.0015452626394107938,\n",
       " 0.9987205266952515,\n",
       " 0.9969440698623657,\n",
       " 0.007431172765791416,\n",
       " 0.0013563204556703568,\n",
       " 0.3348253667354584,\n",
       " 0.002508845180273056,\n",
       " 0.9987311363220215,\n",
       " 0.9984315037727356,\n",
       " 0.9952725768089294,\n",
       " 0.003126277821138501,\n",
       " 0.25120943784713745,\n",
       " 0.0037259634118527174,\n",
       " 0.001574081601575017,\n",
       " 0.997702419757843,\n",
       " 0.07729201018810272,\n",
       " 0.001663460861891508,\n",
       " 0.0019076530588790774]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b381253-e76d-4764-ad3f-caccc67e4610",
   "metadata": {},
   "source": [
    "## Calculate model accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9767e0-39e9-4581-b462-24a0f0f536eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8825\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = (np.array(preds) >= 0.5)\n",
    "accurate = sum(predicted_classes == np.array(y_test).astype(bool))\n",
    "accuracy = accurate/len(y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-roberta_for_longer_texts]",
   "language": "python",
   "name": "conda-env-.conda-roberta_for_longer_texts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
